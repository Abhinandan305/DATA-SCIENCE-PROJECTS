{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c9090c",
   "metadata": {},
   "source": [
    "\n",
    "# Airbnb Review Analysis\n",
    "This notebook performs an analysis of Airbnb reviews to identify patterns in sentiment changes and to perform sequence analysis to prioritize aspects of the product/service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621da70f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Load Data and Preprocess\n",
    "First, load the data and preprocess it by expanding contractions, detecting the language, and cleaning the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef73cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abhinandandas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhinandandas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import unidecode\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Custom contraction mapping\n",
    "CONTRACTION_MAP = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"i'd\": \"i would\", \"i've\": \"i have\",\n",
    "    \"you're\": \"you are\", \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\", \"they're\": \"they are\",\n",
    "    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \"didn't\": \"did not\",\n",
    "    \"don't\": \"do not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"mightn't\": \"might not\", \"mustn't\": \"must not\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        expanded_contraction = contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = match[0] + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/Users/abhinandandas/Downloads/reviews-manc.csv')\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "df.drop(columns={'reviewer_name'}, inplace=True)\n",
    "\n",
    "# Expand contractions\n",
    "df['comments'] = df['comments'].apply(lambda x: expand_contractions(x))\n",
    "\n",
    "# Detect language and filter for English reviews\n",
    "df['Langu'] = [detect(elem) if len(elem) > 50 else 'no' for elem in df['comments']]\n",
    "df = df[df['Langu'] == 'en']\n",
    "\n",
    "# Convert to ASCII\n",
    "df['comments'] = df['comments'].apply(lambda x: unidecode.unidecode(x))\n",
    "\n",
    "# Parse the date column and handle errors\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%d/%m/%Y')\n",
    "        except ValueError:\n",
    "            return pd.NaT\n",
    "\n",
    "df['date'] = df['date'].apply(parse_date)\n",
    "\n",
    "# Filter reviews from 2019 onwards\n",
    "df = df[df['date'].dt.year >= 2019]\n",
    "\n",
    "# Convert comments to lowercase\n",
    "df['comments'] = df['comments'].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d7c2b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Aspect Detection and Sentiment Analysis\n",
    "Using the existing aspect detection code, extract noun phrases and perform sentiment analysis on the comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26de59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to detect noun phrases\n",
    "def detect_noun_phrases(text):\n",
    "    grammar = r\"\"\"\n",
    "      NP: {<JJ><NN.*>+}               # Chunk JJ followed by NN\n",
    "      NP1: {<NN.*>+<JJ>}\n",
    "      NP2: {<NN.*>+<JJ><JJ>*}\n",
    "      NP3: {<NN.*>+<VB.*>*<JJ><JJ>*}\n",
    "      NP4: {<NN.*>+<VB.*>*<RB.*>*<JJ><JJ>*}\n",
    "      NP5: {<PRP$><NN.*>+<JJ><JJ>*}\n",
    "      NP6: {<NN.*>+<VBZ>*<RB>*<JJ><JJ>*}\n",
    "      \"\"\"\n",
    "    parser = nltk.RegexpParser(grammar)\n",
    "    noun_phrases = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        pos_tokens = nltk.pos_tag(words)\n",
    "        parsed_sentence = parser.parse(pos_tokens)\n",
    "        for chunk in parsed_sentence.subtrees():\n",
    "            if chunk.label().startswith('NP'):\n",
    "                noun_phrase = ' '.join(word for word, pos in chunk)\n",
    "                noun_phrases.append(noun_phrase)\n",
    "    return noun_phrases\n",
    "\n",
    "# Apply noun phrase detection\n",
    "df['noun_phrases'] = df['comments'].apply(detect_noun_phrases)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df['sentiment'] = df['comments'].apply(sentiment_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422543b",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Analyze Patterns in Sentiments\n",
    "Analyze the sequence of sentiments in the comments to identify patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002178c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the DataFrame by date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Calculate sentiment change\n",
    "df['sentiment_change'] = df['sentiment'].diff()\n",
    "\n",
    "# Identify patterns\n",
    "def identify_sentiment_pattern(change):\n",
    "    if change > 0:\n",
    "        return 'positive'\n",
    "    elif change < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment_pattern'] = df['sentiment_change'].apply(identify_sentiment_pattern)\n",
    "\n",
    "# Save the results\n",
    "df.to_csv('path_to_save_patterns.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248b561",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Sequence Analysis for Aspect Priority\n",
    "Perform sequence analysis to prioritize the aspects of the product/service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a dictionary to hold aspect sequences\n",
    "aspect_sequences = defaultdict(list)\n",
    "\n",
    "# Extract aspects and their order\n",
    "for index, row in df.iterrows():\n",
    "    aspects = row['noun_phrases']\n",
    "    for i, aspect in enumerate(aspects):\n",
    "        aspect_sequences[aspect].append(i)\n",
    "\n",
    "# Calculate the average position of each aspect\n",
    "aspect_priorities = {aspect: sum(positions)/len(positions) for aspect, positions in aspect_sequences.items()}\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "aspect_priorities_df = pd.DataFrame(list(aspect_priorities.items()), columns=['Aspect', 'Average_Position'])\n",
    "\n",
    "# Sort by average position\n",
    "aspect_priorities_df.sort_values(by='Average_Position', inplace=True)\n",
    "\n",
    "# Save the results\n",
    "aspect_priorities_df.to_csv('path_to_save_aspect_priorities.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
