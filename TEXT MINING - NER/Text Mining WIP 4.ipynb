{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c49f8b6",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0bf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "# from nltk.corpus import wordnet as wn\n",
    "import wordninja\n",
    "from textblob import Word\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Concatenate, Flatten\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# !python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0da84",
   "metadata": {},
   "source": [
    "# Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c598585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(sentence):\n",
    "    # Entity Tag\n",
    "    e1 = re.search(r'<e1>(.*?)</e1>', sentence).group(1).lower()\n",
    "    e2 = re.search(r'<e2>(.*?)</e2>', sentence).group(1).lower()\n",
    "    \n",
    "    if e1 == 'devision':\n",
    "        e1 = 'division'\n",
    "    elif e1 == 'offfender':\n",
    "        e1 = 'offender'\n",
    "        \n",
    "    if e2 == 'devision':\n",
    "        e2 = 'division'\n",
    "    elif e2 == 'offfender':\n",
    "        e2 = 'offender'\n",
    "        \n",
    "    sentence = sentence.replace('>devision<', '>division<').replace('offfender','offender').lower()\n",
    "        \n",
    "    return pd.Series([e1, e2, sentence], index=['e1', 'e2', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4d6ab9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sem_eval_2010_task_8 (/Users/rahulsen/.cache/huggingface/datasets/sem_eval_2010_task_8/default/1.0.0/8545d1995bbbade386acf5c4e2bef5589d8387ae0a93356407dfb54cdb234416)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'the system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>.', 'relation': 3, 'e1': 'configuration', 'e2': 'elements'}\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"sem_eval_2010_task_8\", split = \"train\")\n",
    "train_df = pd.DataFrame(data)\n",
    "train_df[['e1', 'e2','sentence']] = train_df['sentence'].apply(extract_entities)\n",
    "\n",
    "for i in train_df.to_dict('records'):\n",
    "    print (i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "949e8938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell_checker = SpellChecker(distance=1)\n",
    "count = 0\n",
    "\n",
    "def text_to_vectors(text,sentence):\n",
    "    global count\n",
    "    \n",
    "    modified_punctuation_set = ''.join(char for char in string.punctuation)\n",
    "    sentence = re.sub(r'\\s([' + re.escape(modified_punctuation_set) + '])', r'\\1', sentence)\n",
    "    \n",
    "    if text in word2vec_model:\n",
    "        return pd.Series([word2vec_model[text],0, sentence, text], index=['entity_vectors', 'flag', 'corrected_sentence', 'text'])\n",
    "    \n",
    "    elif text.replace('-','') in word2vec_model:\n",
    "        return pd.Series([word2vec_model[text.replace('-','')],0, sentence.replace(text, text.replace('-','')), text.replace('-','')], index=['entity_vectors', 'flag', 'corrected_sentence', 'text'])\n",
    "    \n",
    "    elif text.replace('-',' ').split()[-1] in word2vec_model:\n",
    "        return pd.Series([word2vec_model[text.replace('-',' ').split()[-1]],0, sentence.replace(text, text.replace('-',' ')), text.replace('-',' ')], index=['entity_vectors', 'flag', 'corrected_sentence', 'text'])\n",
    "    \n",
    "    elif text.replace('-',' ').split()[0] in word2vec_model:\n",
    "        return pd.Series([word2vec_model[text.replace('-',' ').split()[0]],0, sentence.replace(text, text.replace('-',' ')), text.replace('-',' ')], index=['entity_vectors', 'flag', 'corrected_sentence', 'text'])\n",
    "    \n",
    "    elif spell_checker.correction(text) in word2vec_model:\n",
    "        return pd.Series([word2vec_model[spell_checker.correction(text)],0, sentence.replace(text, spell_checker.correction(text)), spell_checker.correction(text)], index=['entity_vectors', 'flag', 'corrected_sentence', 'text'])\n",
    "    \n",
    "#     add another elif to break some words into 2 segments\n",
    "    else:\n",
    "        count = count+1\n",
    "        return pd.Series([np.zeros(word2vec_model.vector_size),1, sentence, text], index=['entity_vectors', 'flag', 'corrected_sentence', 'text'])\n",
    "        \n",
    "\n",
    "train_df['corrected_sentence'] = train_df['sentence'].str.replace('<e1>',' ').str.replace('</e1>',' ').str.replace('<e2>',' ').str.replace('</e2>',' ').str.replace('   ', '  ').str.replace('  ', ' ').str.strip()\n",
    "train_df[['entity1_vectors','flag','corrected_sentence','corrected_e1']] = train_df[['e1','corrected_sentence']].apply(lambda x: text_to_vectors(x['e1'],x['corrected_sentence']),axis=1)\n",
    "print (count)\n",
    "train_df[['entity2_vectors','flag','corrected_sentence','corrected_e2']] = train_df[['e2','corrected_sentence']].apply(lambda x: text_to_vectors(x['e2'],x['corrected_sentence']),axis=1)\n",
    "train_df['corrected_sentence'] = train_df['corrected_sentence'].str.replace('-',' ').str.replace('   ', '  ').str.replace('  ', ' ').str.strip()\n",
    "\n",
    "print (count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "89d4e148",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['entity_diff_vector'] = train_df['entity2_vectors']-train_df['entity1_vectors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2c005303",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "councilor\n",
      "indenter\n",
      "nye\n",
      "helicobacter\n",
      "litho\n",
      "spinoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulsen/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rahulsen/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piezos\n",
      "recognizer\n",
      "chai\n",
      "muscularis\n",
      "floodwaters\n",
      "mage\n",
      "santur\n",
      "gamers\n",
      "natron\n",
      "installer\n",
      "audiophile\n",
      "opioids\n",
      "acetonitrile\n",
      "azeotrope\n",
      "app\n",
      "eyrar\n",
      "joey\n",
      "toolkits\n",
      "orienteering\n",
      "biodiesel\n",
      "humidifier\n",
      "motherboard\n",
      "login\n",
      "shockwaves\n",
      "entrepreneurship\n",
      "supercell\n",
      "bytecodes\n",
      "slideshows\n",
      "blogosphere\n",
      "stovetop\n",
      "toolkit\n",
      "katanas\n",
      "malware\n",
      "townhouse\n",
      "automotives\n",
      "maireeners\n",
      "papillomavirus\n",
      "photomultiplier\n",
      "cementation\n",
      "apps\n",
      "microcontroller\n",
      "quilters\n",
      "mesophyll\n",
      "worldview\n",
      "firefighting\n",
      "folksingers\n",
      "groundwater\n",
      "dialysate\n",
      "groundwater\n",
      "geodesist\n",
      "sparrowhawks\n",
      "biochemicals\n",
      "masterplan\n",
      "iphone\n",
      "ceasefire\n",
      "politicization\n",
      "anisotropic etching\n",
      "skeletal remains\n",
      "trapdoors\n",
      "stylesheet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>relation</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>corrected_sentence</th>\n",
       "      <th>entity1_vectors</th>\n",
       "      <th>flag</th>\n",
       "      <th>corrected_e1</th>\n",
       "      <th>entity2_vectors</th>\n",
       "      <th>corrected_e2</th>\n",
       "      <th>...</th>\n",
       "      <th>e2_post_token</th>\n",
       "      <th>e2_start</th>\n",
       "      <th>e2_end</th>\n",
       "      <th>e2_position</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_prep</th>\n",
       "      <th>num_adj</th>\n",
       "      <th>num_words_btwn</th>\n",
       "      <th>bert_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>3</td>\n",
       "      <td>configuration</td>\n",
       "      <td>elements</td>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>[0.13964844, 0.07373047, -0.037841797, 0.10302...</td>\n",
       "      <td>0</td>\n",
       "      <td>configuration</td>\n",
       "      <td>[0.10888672, 0.1796875, 0.107910156, 0.0198974...</td>\n",
       "      <td>elements</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>98</td>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the &lt;e1&gt;child&lt;/e1&gt; was carefully wrapped and b...</td>\n",
       "      <td>18</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "      <td>[0.16503906, -0.063964844, -0.0017852783, 0.18...</td>\n",
       "      <td>0</td>\n",
       "      <td>child</td>\n",
       "      <td>[0.15917969, 0.076171875, 0.01953125, 0.21875,...</td>\n",
       "      <td>cradle</td>\n",
       "      <td>...</td>\n",
       "      <td>by</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the &lt;e1&gt;author&lt;/e1&gt; of a keygen uses a &lt;e2&gt;dis...</td>\n",
       "      <td>11</td>\n",
       "      <td>author</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>the author of a keygen uses a disassembler to ...</td>\n",
       "      <td>[0.12988281, -0.140625, 0.041748047, 0.0927734...</td>\n",
       "      <td>0</td>\n",
       "      <td>author</td>\n",
       "      <td>[0.15332031, 0.095703125, 0.010925293, 0.05639...</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>...</td>\n",
       "      <td>to</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>the author of a keygen uses a disassembler to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a misty &lt;e1&gt;ridge&lt;/e1&gt; uprises from the &lt;e2&gt;su...</td>\n",
       "      <td>18</td>\n",
       "      <td>ridge</td>\n",
       "      <td>surge</td>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "      <td>[-0.09472656, -0.059814453, -0.203125, 0.06298...</td>\n",
       "      <td>0</td>\n",
       "      <td>ridge</td>\n",
       "      <td>[0.104003906, 0.13574219, -0.10644531, -0.0260...</td>\n",
       "      <td>surge</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the &lt;e1&gt;student&lt;/e1&gt; &lt;e2&gt;association&lt;/e2&gt; is t...</td>\n",
       "      <td>12</td>\n",
       "      <td>student</td>\n",
       "      <td>association</td>\n",
       "      <td>the student association is the voice of the un...</td>\n",
       "      <td>[0.036865234, 0.020141602, 0.22167969, 0.15527...</td>\n",
       "      <td>0</td>\n",
       "      <td>student</td>\n",
       "      <td>[-0.20703125, -0.28710938, 0.03564453, -0.0859...</td>\n",
       "      <td>association</td>\n",
       "      <td>...</td>\n",
       "      <td>is</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>the student association is the voice of the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>when the &lt;e1&gt;notice&lt;/e1&gt; is sent by &lt;e2&gt;fax&lt;/e...</td>\n",
       "      <td>18</td>\n",
       "      <td>notice</td>\n",
       "      <td>fax</td>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "      <td>[-0.19824219, 0.08984375, 0.20898438, -0.16699...</td>\n",
       "      <td>0</td>\n",
       "      <td>notice</td>\n",
       "      <td>[-0.26171875, -0.039794922, 0.12890625, -0.097...</td>\n",
       "      <td>fax</td>\n",
       "      <td>...</td>\n",
       "      <td>,</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>the &lt;e1&gt;herbicide&lt;/e1&gt; is derived from a natur...</td>\n",
       "      <td>8</td>\n",
       "      <td>herbicide</td>\n",
       "      <td>antibiotic</td>\n",
       "      <td>the herbicide is derived from a natural antibi...</td>\n",
       "      <td>[0.20019531, 0.29492188, -0.115234375, 0.26953...</td>\n",
       "      <td>0</td>\n",
       "      <td>herbicide</td>\n",
       "      <td>[-0.14550781, 0.20019531, 0.07861328, -0.03955...</td>\n",
       "      <td>antibiotic</td>\n",
       "      <td>...</td>\n",
       "      <td>,</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>the herbicide is derived from a natural antibi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>to test this, we placed a kitchen &lt;e1&gt;match&lt;/e...</td>\n",
       "      <td>6</td>\n",
       "      <td>match</td>\n",
       "      <td>jar</td>\n",
       "      <td>to test this, we placed a kitchen match in the...</td>\n",
       "      <td>[-0.15527344, 0.025024414, 0.064941406, -0.124...</td>\n",
       "      <td>0</td>\n",
       "      <td>match</td>\n",
       "      <td>[0.11621094, 0.11621094, 0.043945312, 0.267578...</td>\n",
       "      <td>jar</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>to test this, we placed a kitchen match in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>18</td>\n",
       "      <td>farmers</td>\n",
       "      <td>market</td>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>[0.17578125, 0.07470703, -0.24707031, 0.233398...</td>\n",
       "      <td>0</td>\n",
       "      <td>farmers</td>\n",
       "      <td>[-0.15625, -0.087890625, -0.22949219, -0.23144...</td>\n",
       "      <td>market</td>\n",
       "      <td>...</td>\n",
       "      <td>into</td>\n",
       "      <td>95</td>\n",
       "      <td>101</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>the &lt;e1&gt;surgeon&lt;/e1&gt; cuts a small &lt;e2&gt;hole&lt;/e2...</td>\n",
       "      <td>17</td>\n",
       "      <td>surgeon</td>\n",
       "      <td>hole</td>\n",
       "      <td>the surgeon cuts a small hole in the skull and...</td>\n",
       "      <td>[0.100097656, 0.203125, 0.15527344, -0.0612792...</td>\n",
       "      <td>0</td>\n",
       "      <td>surgeon</td>\n",
       "      <td>[-0.14257812, 0.12597656, 0.15527344, -0.03466...</td>\n",
       "      <td>hole</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the surgeon cuts a small hole in the skull and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  relation  \\\n",
       "0     the system as described above has its greatest...         3   \n",
       "1     the <e1>child</e1> was carefully wrapped and b...        18   \n",
       "2     the <e1>author</e1> of a keygen uses a <e2>dis...        11   \n",
       "3     a misty <e1>ridge</e1> uprises from the <e2>su...        18   \n",
       "4     the <e1>student</e1> <e2>association</e2> is t...        12   \n",
       "...                                                 ...       ...   \n",
       "7995  when the <e1>notice</e1> is sent by <e2>fax</e...        18   \n",
       "7996  the <e1>herbicide</e1> is derived from a natur...         8   \n",
       "7997  to test this, we placed a kitchen <e1>match</e...         6   \n",
       "7998  the farmers and city officials in the region h...        18   \n",
       "7999  the <e1>surgeon</e1> cuts a small <e2>hole</e2...        17   \n",
       "\n",
       "                 e1            e2  \\\n",
       "0     configuration      elements   \n",
       "1             child        cradle   \n",
       "2            author  disassembler   \n",
       "3             ridge         surge   \n",
       "4           student   association   \n",
       "...             ...           ...   \n",
       "7995         notice           fax   \n",
       "7996      herbicide    antibiotic   \n",
       "7997          match           jar   \n",
       "7998        farmers        market   \n",
       "7999        surgeon          hole   \n",
       "\n",
       "                                     corrected_sentence  \\\n",
       "0     the system as described above has its greatest...   \n",
       "1     the child was carefully wrapped and bound into...   \n",
       "2     the author of a keygen uses a disassembler to ...   \n",
       "3                 a misty ridge uprises from the surge.   \n",
       "4     the student association is the voice of the un...   \n",
       "...                                                 ...   \n",
       "7995  when the notice is sent by fax, the notice is ...   \n",
       "7996  the herbicide is derived from a natural antibi...   \n",
       "7997  to test this, we placed a kitchen match in the...   \n",
       "7998  the farmers and city officials in the region h...   \n",
       "7999  the surgeon cuts a small hole in the skull and...   \n",
       "\n",
       "                                        entity1_vectors  flag   corrected_e1  \\\n",
       "0     [0.13964844, 0.07373047, -0.037841797, 0.10302...     0  configuration   \n",
       "1     [0.16503906, -0.063964844, -0.0017852783, 0.18...     0          child   \n",
       "2     [0.12988281, -0.140625, 0.041748047, 0.0927734...     0         author   \n",
       "3     [-0.09472656, -0.059814453, -0.203125, 0.06298...     0          ridge   \n",
       "4     [0.036865234, 0.020141602, 0.22167969, 0.15527...     0        student   \n",
       "...                                                 ...   ...            ...   \n",
       "7995  [-0.19824219, 0.08984375, 0.20898438, -0.16699...     0         notice   \n",
       "7996  [0.20019531, 0.29492188, -0.115234375, 0.26953...     0      herbicide   \n",
       "7997  [-0.15527344, 0.025024414, 0.064941406, -0.124...     0          match   \n",
       "7998  [0.17578125, 0.07470703, -0.24707031, 0.233398...     0        farmers   \n",
       "7999  [0.100097656, 0.203125, 0.15527344, -0.0612792...     0        surgeon   \n",
       "\n",
       "                                        entity2_vectors  corrected_e2  ...  \\\n",
       "0     [0.10888672, 0.1796875, 0.107910156, 0.0198974...      elements  ...   \n",
       "1     [0.15917969, 0.076171875, 0.01953125, 0.21875,...        cradle  ...   \n",
       "2     [0.15332031, 0.095703125, 0.010925293, 0.05639...  disassembler  ...   \n",
       "3     [0.104003906, 0.13574219, -0.10644531, -0.0260...         surge  ...   \n",
       "4     [-0.20703125, -0.28710938, 0.03564453, -0.0859...   association  ...   \n",
       "...                                                 ...           ...  ...   \n",
       "7995  [-0.26171875, -0.039794922, 0.12890625, -0.097...           fax  ...   \n",
       "7996  [-0.14550781, 0.20019531, 0.07861328, -0.03955...    antibiotic  ...   \n",
       "7997  [0.11621094, 0.11621094, 0.043945312, 0.267578...           jar  ...   \n",
       "7998  [-0.15625, -0.087890625, -0.22949219, -0.23144...        market  ...   \n",
       "7999  [-0.14257812, 0.12597656, 0.15527344, -0.03466...          hole  ...   \n",
       "\n",
       "     e2_post_token e2_start e2_end e2_position num_verbs num_nouns num_prep  \\\n",
       "0                .       98    106          16         0         2        1   \n",
       "1               by       51     57          10         2         1        1   \n",
       "2               to       30     42           8         1         2        1   \n",
       "3                .       31     36           7         0         2        1   \n",
       "4               is       12     23           3         0         4        3   \n",
       "...            ...      ...    ...         ...       ...       ...      ...   \n",
       "7995             ,       27     30           7         4         3        3   \n",
       "7996             ,       40     50           8         1         1        1   \n",
       "7997             .       47     50          12         2         3        2   \n",
       "7998          into       95    101          19         3        11        2   \n",
       "7999            in       25     29           6         1         1        0   \n",
       "\n",
       "     num_adj num_words_btwn                                      bert_sentence  \n",
       "0          0              3  the system as described above has its greatest...  \n",
       "1          0              8  the child was carefully wrapped and bound into...  \n",
       "2          0              6  the author of a keygen uses a disassembler to ...  \n",
       "3          0              4              a misty ridge uprises from the surge.  \n",
       "4          0             13  the student association is the voice of the un...  \n",
       "...      ...            ...                                                ...  \n",
       "7995       0             16  when the notice is sent by fax, the notice is ...  \n",
       "7996       1              6  the herbicide is derived from a natural antibi...  \n",
       "7997       0             12  to test this, we placed a kitchen match in the...  \n",
       "7998       1             26  the farmers and city officials in the region h...  \n",
       "7999       1              4  the surgeon cuts a small hole in the skull and...  \n",
       "\n",
       "[8000 rows x 50 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_synsets(word):\n",
    "    words = word.replace('-',' ').split(' ')\n",
    "    new_words = []\n",
    "    for i in words:\n",
    "        new_words = new_words + wordninja.split(nlp(i)[0].lemma_)\n",
    "    synsets = []\n",
    "    for item in new_words:\n",
    "        item_synset = wordnet.synsets(item)\n",
    "        final_item_synset = []\n",
    "        for synset in item_synset:\n",
    "            if synset.name().split('.')[1]=='n':\n",
    "                final_item_synset.append(synset)\n",
    "        if len(synsets)>0 and len(final_item_synset)==0:\n",
    "            continue\n",
    "        synsets.append(final_item_synset)\n",
    "    return synsets[-1]\n",
    "\n",
    "def get_word_pos(word):\n",
    "    pos_tag = nltk.pos_tag([word])[0][1]\n",
    "    if pos_tag.startswith('N'):  # Noun\n",
    "        return 'n'\n",
    "    elif pos_tag.startswith('V'):  # Verb\n",
    "        return 'v'\n",
    "    elif pos_tag.startswith('J'):  # Adjective\n",
    "        return 'a'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_words_by_pos(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # Filter tokens by POS (noun, verb, adjective)\n",
    "    filtered_tokens = [token for token in tokens if get_word_pos(token) is not None]\n",
    "    return filtered_tokens\n",
    "\n",
    "def disambiguate_word_in_sentence(sentence, word):\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    tokens = filter_words_by_pos(sentence.replace(word,''))\n",
    "    \n",
    "    # Get synsets for the word\n",
    "    synsets = wordnet.synsets(word.replace(' ','_'))\n",
    "    if len(synsets)==0:\n",
    "        synsets = get_synsets(word)\n",
    "        \n",
    "    # Initialize a dictionary to store scores for each synset\n",
    "    scores = {}\n",
    "    if len(synsets)==0:\n",
    "        print (word)\n",
    "    \n",
    "    # Calculate scores for each synset based on word embeddings similarity\n",
    "    for synset in synsets:\n",
    "        gloss = synset.definition()\n",
    "        gloss_tokens = filter_words_by_pos(gloss)\n",
    "\n",
    "        # Calculate average embedding for gloss tokens\n",
    "        gloss_embeddings = [word2vec_model[token] for token in gloss_tokens if token in word2vec_model]\n",
    "        avg_gloss_embedding = np.mean(gloss_embeddings, axis=0)\n",
    "        # Calculate similarity score based on cosine similarity between avg_gloss_embedding and each token in sentence\n",
    "        similarity_scores = [np.dot(avg_gloss_embedding, word2vec_model[token]) /\n",
    "                             (np.linalg.norm(avg_gloss_embedding) * np.linalg.norm(word2vec_model[token]))\n",
    "                             for token in tokens if token in word2vec_model]\n",
    "        scores[synset] = np.mean(similarity_scores)\n",
    "    \n",
    "    # Get the synset with the highest score\n",
    "    if len(synsets)>0:\n",
    "        best_synset = max(scores, key=scores.get)\n",
    "    else:\n",
    "        best_synset = wordnet.synsets('unavailable')[0]\n",
    "    \n",
    "    return best_synset\n",
    "\n",
    "def get_hypernym(synset1, synset2):\n",
    "    common_hypernym = synset1.lowest_common_hypernyms(synset2)\n",
    "    return common_hypernym[0].lemmas()[0].name() if common_hypernym else None\n",
    "\n",
    "def increase_flag(e1_synset,e2_synset,flag):\n",
    "    if e1_synset.name().split('.')[0]=='unavailable':\n",
    "        flag = flag+1\n",
    "    if e2_synset.name().split('.')[0]=='unavailable':\n",
    "        flag = flag+1\n",
    "    return flag\n",
    "\n",
    "train_df['e1_synset'] = np.vectorize(disambiguate_word_in_sentence)(train_df['corrected_sentence'],train_df['corrected_e1'])\n",
    "train_df['e2_synset'] = np.vectorize(disambiguate_word_in_sentence)(train_df['corrected_sentence'],train_df['corrected_e2'])\n",
    "train_df['e1_supersense'] = train_df.apply(lambda x: x['e1_synset'].lexname().split('.')[-1],axis=1)\n",
    "train_df['e2_supersense'] = train_df.apply(lambda x: x['e2_synset'].lexname().split('.')[-1],axis=1)\n",
    "train_df['hypernym'] = np.vectorize(get_hypernym)(train_df['e1_synset'],train_df['e2_synset'])\n",
    "train_df['e1_definition'] = train_df.apply(lambda x: x['e1_synset'].definition(),axis=1)\n",
    "train_df['e2_definition'] = train_df.apply(lambda x: x['e2_synset'].definition(),axis=1)\n",
    "\n",
    "train_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a7c0a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 'biochemicals' into 'bio' and 'chemicals'\n"
     ]
    }
   ],
   "source": [
    "def split_word(word):\n",
    "    for i in range(1, len(word)):\n",
    "        prefix = word[:i]\n",
    "        suffix = word[i:]\n",
    "        if prefix in word2vec_model and suffix in word2vec_model:\n",
    "            return prefix, suffix\n",
    "    return None, None\n",
    "\n",
    "word = \"biochemicals\"\n",
    "prefix, suffix = split_word(word)\n",
    "if prefix and suffix:\n",
    "    print(f\"Splitting '{word}' into '{prefix}' and '{suffix}'\")\n",
    "else:\n",
    "    print(f\"Cannot split '{word}' into two meaningful words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "60d6f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['flag'] = np.vectorize(increase_flag)(train_df['e1_synset'],train_df['e2_synset'],train_df['flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "271dbb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>relation</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>corrected_sentence</th>\n",
       "      <th>entity1_vectors</th>\n",
       "      <th>flag</th>\n",
       "      <th>corrected_e1</th>\n",
       "      <th>entity2_vectors</th>\n",
       "      <th>corrected_e2</th>\n",
       "      <th>...</th>\n",
       "      <th>e2_post_token</th>\n",
       "      <th>e2_start</th>\n",
       "      <th>e2_end</th>\n",
       "      <th>e2_position</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_prep</th>\n",
       "      <th>num_adj</th>\n",
       "      <th>num_words_btwn</th>\n",
       "      <th>bert_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>3</td>\n",
       "      <td>configuration</td>\n",
       "      <td>elements</td>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>[0.13964844, 0.07373047, -0.037841797, 0.10302...</td>\n",
       "      <td>0</td>\n",
       "      <td>configuration</td>\n",
       "      <td>[0.10888672, 0.1796875, 0.107910156, 0.0198974...</td>\n",
       "      <td>elements</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>98</td>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the &lt;e1&gt;child&lt;/e1&gt; was carefully wrapped and b...</td>\n",
       "      <td>18</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "      <td>[0.16503906, -0.063964844, -0.0017852783, 0.18...</td>\n",
       "      <td>0</td>\n",
       "      <td>child</td>\n",
       "      <td>[0.15917969, 0.076171875, 0.01953125, 0.21875,...</td>\n",
       "      <td>cradle</td>\n",
       "      <td>...</td>\n",
       "      <td>by</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the &lt;e1&gt;author&lt;/e1&gt; of a keygen uses a &lt;e2&gt;dis...</td>\n",
       "      <td>11</td>\n",
       "      <td>author</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>the author of a keygen uses a disassembler to ...</td>\n",
       "      <td>[0.12988281, -0.140625, 0.041748047, 0.0927734...</td>\n",
       "      <td>0</td>\n",
       "      <td>author</td>\n",
       "      <td>[0.15332031, 0.095703125, 0.010925293, 0.05639...</td>\n",
       "      <td>disassembler</td>\n",
       "      <td>...</td>\n",
       "      <td>to</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>the author of a keygen uses a disassembler to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a misty &lt;e1&gt;ridge&lt;/e1&gt; uprises from the &lt;e2&gt;su...</td>\n",
       "      <td>18</td>\n",
       "      <td>ridge</td>\n",
       "      <td>surge</td>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "      <td>[-0.09472656, -0.059814453, -0.203125, 0.06298...</td>\n",
       "      <td>0</td>\n",
       "      <td>ridge</td>\n",
       "      <td>[0.104003906, 0.13574219, -0.10644531, -0.0260...</td>\n",
       "      <td>surge</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the &lt;e1&gt;student&lt;/e1&gt; &lt;e2&gt;association&lt;/e2&gt; is t...</td>\n",
       "      <td>12</td>\n",
       "      <td>student</td>\n",
       "      <td>association</td>\n",
       "      <td>the student association is the voice of the un...</td>\n",
       "      <td>[0.036865234, 0.020141602, 0.22167969, 0.15527...</td>\n",
       "      <td>0</td>\n",
       "      <td>student</td>\n",
       "      <td>[-0.20703125, -0.28710938, 0.03564453, -0.0859...</td>\n",
       "      <td>association</td>\n",
       "      <td>...</td>\n",
       "      <td>is</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>the student association is the voice of the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>when the &lt;e1&gt;notice&lt;/e1&gt; is sent by &lt;e2&gt;fax&lt;/e...</td>\n",
       "      <td>18</td>\n",
       "      <td>notice</td>\n",
       "      <td>fax</td>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "      <td>[-0.19824219, 0.08984375, 0.20898438, -0.16699...</td>\n",
       "      <td>0</td>\n",
       "      <td>notice</td>\n",
       "      <td>[-0.26171875, -0.039794922, 0.12890625, -0.097...</td>\n",
       "      <td>fax</td>\n",
       "      <td>...</td>\n",
       "      <td>,</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>the &lt;e1&gt;herbicide&lt;/e1&gt; is derived from a natur...</td>\n",
       "      <td>8</td>\n",
       "      <td>herbicide</td>\n",
       "      <td>antibiotic</td>\n",
       "      <td>the herbicide is derived from a natural antibi...</td>\n",
       "      <td>[0.20019531, 0.29492188, -0.115234375, 0.26953...</td>\n",
       "      <td>0</td>\n",
       "      <td>herbicide</td>\n",
       "      <td>[-0.14550781, 0.20019531, 0.07861328, -0.03955...</td>\n",
       "      <td>antibiotic</td>\n",
       "      <td>...</td>\n",
       "      <td>,</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>the herbicide is derived from a natural antibi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>to test this, we placed a kitchen &lt;e1&gt;match&lt;/e...</td>\n",
       "      <td>6</td>\n",
       "      <td>match</td>\n",
       "      <td>jar</td>\n",
       "      <td>to test this, we placed a kitchen match in the...</td>\n",
       "      <td>[-0.15527344, 0.025024414, 0.064941406, -0.124...</td>\n",
       "      <td>0</td>\n",
       "      <td>match</td>\n",
       "      <td>[0.11621094, 0.11621094, 0.043945312, 0.267578...</td>\n",
       "      <td>jar</td>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>to test this, we placed a kitchen match in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>18</td>\n",
       "      <td>farmers</td>\n",
       "      <td>market</td>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>[0.17578125, 0.07470703, -0.24707031, 0.233398...</td>\n",
       "      <td>0</td>\n",
       "      <td>farmers</td>\n",
       "      <td>[-0.15625, -0.087890625, -0.22949219, -0.23144...</td>\n",
       "      <td>market</td>\n",
       "      <td>...</td>\n",
       "      <td>into</td>\n",
       "      <td>95</td>\n",
       "      <td>101</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>the &lt;e1&gt;surgeon&lt;/e1&gt; cuts a small &lt;e2&gt;hole&lt;/e2...</td>\n",
       "      <td>17</td>\n",
       "      <td>surgeon</td>\n",
       "      <td>hole</td>\n",
       "      <td>the surgeon cuts a small hole in the skull and...</td>\n",
       "      <td>[0.100097656, 0.203125, 0.15527344, -0.0612792...</td>\n",
       "      <td>0</td>\n",
       "      <td>surgeon</td>\n",
       "      <td>[-0.14257812, 0.12597656, 0.15527344, -0.03466...</td>\n",
       "      <td>hole</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>the surgeon cuts a small hole in the skull and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  relation  \\\n",
       "0     the system as described above has its greatest...         3   \n",
       "1     the <e1>child</e1> was carefully wrapped and b...        18   \n",
       "2     the <e1>author</e1> of a keygen uses a <e2>dis...        11   \n",
       "3     a misty <e1>ridge</e1> uprises from the <e2>su...        18   \n",
       "4     the <e1>student</e1> <e2>association</e2> is t...        12   \n",
       "...                                                 ...       ...   \n",
       "7995  when the <e1>notice</e1> is sent by <e2>fax</e...        18   \n",
       "7996  the <e1>herbicide</e1> is derived from a natur...         8   \n",
       "7997  to test this, we placed a kitchen <e1>match</e...         6   \n",
       "7998  the farmers and city officials in the region h...        18   \n",
       "7999  the <e1>surgeon</e1> cuts a small <e2>hole</e2...        17   \n",
       "\n",
       "                 e1            e2  \\\n",
       "0     configuration      elements   \n",
       "1             child        cradle   \n",
       "2            author  disassembler   \n",
       "3             ridge         surge   \n",
       "4           student   association   \n",
       "...             ...           ...   \n",
       "7995         notice           fax   \n",
       "7996      herbicide    antibiotic   \n",
       "7997          match           jar   \n",
       "7998        farmers        market   \n",
       "7999        surgeon          hole   \n",
       "\n",
       "                                     corrected_sentence  \\\n",
       "0     the system as described above has its greatest...   \n",
       "1     the child was carefully wrapped and bound into...   \n",
       "2     the author of a keygen uses a disassembler to ...   \n",
       "3                 a misty ridge uprises from the surge.   \n",
       "4     the student association is the voice of the un...   \n",
       "...                                                 ...   \n",
       "7995  when the notice is sent by fax, the notice is ...   \n",
       "7996  the herbicide is derived from a natural antibi...   \n",
       "7997  to test this, we placed a kitchen match in the...   \n",
       "7998  the farmers and city officials in the region h...   \n",
       "7999  the surgeon cuts a small hole in the skull and...   \n",
       "\n",
       "                                        entity1_vectors  flag   corrected_e1  \\\n",
       "0     [0.13964844, 0.07373047, -0.037841797, 0.10302...     0  configuration   \n",
       "1     [0.16503906, -0.063964844, -0.0017852783, 0.18...     0          child   \n",
       "2     [0.12988281, -0.140625, 0.041748047, 0.0927734...     0         author   \n",
       "3     [-0.09472656, -0.059814453, -0.203125, 0.06298...     0          ridge   \n",
       "4     [0.036865234, 0.020141602, 0.22167969, 0.15527...     0        student   \n",
       "...                                                 ...   ...            ...   \n",
       "7995  [-0.19824219, 0.08984375, 0.20898438, -0.16699...     0         notice   \n",
       "7996  [0.20019531, 0.29492188, -0.115234375, 0.26953...     0      herbicide   \n",
       "7997  [-0.15527344, 0.025024414, 0.064941406, -0.124...     0          match   \n",
       "7998  [0.17578125, 0.07470703, -0.24707031, 0.233398...     0        farmers   \n",
       "7999  [0.100097656, 0.203125, 0.15527344, -0.0612792...     0        surgeon   \n",
       "\n",
       "                                        entity2_vectors  corrected_e2  ...  \\\n",
       "0     [0.10888672, 0.1796875, 0.107910156, 0.0198974...      elements  ...   \n",
       "1     [0.15917969, 0.076171875, 0.01953125, 0.21875,...        cradle  ...   \n",
       "2     [0.15332031, 0.095703125, 0.010925293, 0.05639...  disassembler  ...   \n",
       "3     [0.104003906, 0.13574219, -0.10644531, -0.0260...         surge  ...   \n",
       "4     [-0.20703125, -0.28710938, 0.03564453, -0.0859...   association  ...   \n",
       "...                                                 ...           ...  ...   \n",
       "7995  [-0.26171875, -0.039794922, 0.12890625, -0.097...           fax  ...   \n",
       "7996  [-0.14550781, 0.20019531, 0.07861328, -0.03955...    antibiotic  ...   \n",
       "7997  [0.11621094, 0.11621094, 0.043945312, 0.267578...           jar  ...   \n",
       "7998  [-0.15625, -0.087890625, -0.22949219, -0.23144...        market  ...   \n",
       "7999  [-0.14257812, 0.12597656, 0.15527344, -0.03466...          hole  ...   \n",
       "\n",
       "     e2_post_token e2_start e2_end e2_position num_verbs num_nouns num_prep  \\\n",
       "0                .       98    106          16         0         2        1   \n",
       "1               by       51     57          10         2         1        1   \n",
       "2               to       30     42           8         1         2        1   \n",
       "3                .       31     36           7         0         2        1   \n",
       "4               is       12     23           3         0         4        3   \n",
       "...            ...      ...    ...         ...       ...       ...      ...   \n",
       "7995             ,       27     30           7         4         3        3   \n",
       "7996             ,       40     50           8         1         1        1   \n",
       "7997             .       47     50          12         2         3        2   \n",
       "7998          into       95    101          19         3        11        2   \n",
       "7999            in       25     29           6         1         1        0   \n",
       "\n",
       "     num_adj num_words_btwn                                      bert_sentence  \n",
       "0          0              3  the system as described above has its greatest...  \n",
       "1          0              8  the child was carefully wrapped and bound into...  \n",
       "2          0              6  the author of a keygen uses a disassembler to ...  \n",
       "3          0              4              a misty ridge uprises from the surge.  \n",
       "4          0             13  the student association is the voice of the un...  \n",
       "...      ...            ...                                                ...  \n",
       "7995       0             16  when the notice is sent by fax, the notice is ...  \n",
       "7996       1              6  the herbicide is derived from a natural antibi...  \n",
       "7997       0             12  to test this, we placed a kitchen match in the...  \n",
       "7998       1             26  the farmers and city officials in the region h...  \n",
       "7999       1              4  the surgeon cuts a small hole in the skull and...  \n",
       "\n",
       "[8000 rows x 50 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features( e1, e2, old_sentence):\n",
    "\n",
    "    old_sentence = old_sentence.replace('<e1>','').replace('</e1>','').replace('<e2>','').replace('</e2>','')\n",
    "    old_doc = nlp(old_sentence)\n",
    "\n",
    "    e1_pos, e2_pos = None, None\n",
    "    e1_dep_token, e2_dep_token = 'NA.', 'NA.'\n",
    "    e1_prev_token, e2_prev_token = 'NA.', 'NA.'\n",
    "    e1_post_token, e2_post_token = 'NA.', 'NA.'\n",
    "    \n",
    "    e1_dep_noun, e1_dep_adj, e1_dep_verb, e1_dep_prep, e1_dep_subj, e1_dep_obj = 0, 0, 0, 0, 0, 0\n",
    "    e2_dep_noun, e2_dep_adj, e2_dep_verb, e2_dep_prep, e2_dep_subj, e2_dep_obj = 0, 0, 0, 0, 0, 0\n",
    "    num_verbs, num_nouns, num_prep, num_adj, num_words_btwn = 0, 0, 0, 0, 0 \n",
    "    start_inbetween_count = False\n",
    "    \n",
    "    word_count = 0\n",
    "    e1_position_set = e2_position_set = False\n",
    "    e1_post_memory = e2_post_memory = False\n",
    "    memory = 'NA.'\n",
    "    e1_position = e2_position = -1\n",
    "    for token in old_doc:\n",
    "        word_count = word_count + 1\n",
    "        \n",
    "        if e1_post_memory == True:\n",
    "            e1_post_token = str(token)\n",
    "        e1_post_memory = False\n",
    "        \n",
    "        if e2_post_memory == True:\n",
    "            e2_post_token = str(token)\n",
    "        e2_post_memory = False\n",
    "        \n",
    "        if str(token) in e1.split():\n",
    "            e1_pos = token.pos_\n",
    "            e1_dep_noun = 1 if token.dep_[0]=='n' else 0\n",
    "            e1_dep_adj = 1 if token.dep_[0]=='a' else 0\n",
    "            e1_dep_verb = 1 if token.dep_[0]=='v' else 0\n",
    "            e1_dep_prep = 1 if token.dep_[0]=='p' else 0\n",
    "            e1_dep_subj = 1 if 'subj' in token.dep_ else 0\n",
    "            e1_dep_obj = 1 if 'obj' in token.dep_ else 0\n",
    "            e1_dep_token = str(token.head)\n",
    "            \n",
    "            if e1_prev_token == 'NA.':\n",
    "                e1_prev_token = memory\n",
    "                \n",
    "            if e1_position_set == False:\n",
    "                e1_position = word_count\n",
    "                e1_position_set = True\n",
    "                \n",
    "            e1_post_memory = True\n",
    "            \n",
    "            \n",
    "        if str(token) in e2.split(): \n",
    "            e2_pos = token.pos_\n",
    "            e2_dep_noun = 1 if token.dep_[0]=='n' else 0\n",
    "            e2_dep_adj = 1 if token.dep_[0]=='a' else 0\n",
    "            e2_dep_verb = 1 if token.dep_[0]=='v' else 0\n",
    "            e2_dep_prep = 1 if token.dep_[0]=='p' else 0\n",
    "            e2_dep_subj = 1 if 'subj' in token.dep_ else 0\n",
    "            e2_dep_obj = 1 if 'obj' in token.dep_ else 0\n",
    "            e2_dep_token = str(token.head)\n",
    "            \n",
    "            if e2_prev_token == 'NA.':\n",
    "                e2_prev_token = memory\n",
    "                \n",
    "            if e2_position_set == False:\n",
    "                e2_position = word_count\n",
    "                e2_position_set = True\n",
    "                \n",
    "            e2_post_memory = True\n",
    "            \n",
    "        if (str(token) in [e1.split()[-1],e2.split()[-1]] and start_inbetween_count==False) or (str(token) in [e1.split()[0],e2.split()[0]] and start_inbetween_count==True):\n",
    "            start_inbetween_count = not(start_inbetween_count)\n",
    "            \n",
    "        if start_inbetween_count==True:\n",
    "            num_verbs = num_verbs+1 if token.pos_=='VERB' else num_verbs\n",
    "            num_nouns = num_nouns+1 if token.pos_=='NOUN' else num_nouns\n",
    "            num_prep = num_prep+1 if token.pos_=='ADP' else num_prep\n",
    "            num_adj = num_adj+1 if token.pos_=='ADJ' else num_adj\n",
    "            num_words_btwn = num_words_btwn+1 \n",
    "            \n",
    "        memory = str(token)\n",
    "\n",
    "    \n",
    "    e1_start = old_sentence.find(e1)\n",
    "    e1_end = e1_start + len(e1)\n",
    "    e2_start = old_sentence.find(e2)\n",
    "    e2_end = e2_start + len(e2)\n",
    "\n",
    "    \n",
    "    return pd.Series([e1_pos, e1_dep_noun, e1_dep_adj, e1_dep_verb, e1_dep_prep, e1_dep_subj, e1_dep_obj, e1_dep_token, e1_prev_token, e1_post_token, e1_start, e1_end, e1_position, e2_pos, e2_dep_noun, e2_dep_adj, e2_dep_verb, e2_dep_prep, e2_dep_subj, e2_dep_obj, e2_dep_token, e2_prev_token, e2_post_token, e2_start, e2_end, e2_position, num_verbs, num_nouns, num_prep, num_adj, num_words_btwn], index=['e1_pos', 'e1_dep_noun', 'e1_dep_adj', 'e1_dep_verb', 'e1_dep_prep', 'e1_dep_subj', 'e1_dep_obj', 'e1_dep_token', 'e1_prev_token', 'e1_post_token', 'e1_start', 'e1_end', 'e1_position', 'e2_pos', 'e2_dep_noun', 'e2_dep_adj', 'e2_dep_verb', 'e2_dep_prep', 'e2_dep_subj', 'e2_dep_obj', 'e2_dep_token', 'e2_prev_token', 'e2_post_token', 'e2_start', 'e2_end', 'e2_position', 'num_verbs', 'num_nouns', 'num_prep', 'num_adj', 'num_words_btwn'])\n",
    "\n",
    "\n",
    "\n",
    "def get_entity_type(text):\n",
    "    doc = nlp(text)\n",
    "    entity_types = [ent.label_ for ent in doc.ents]\n",
    "    return entity_types[0] if entity_types else None\n",
    "\n",
    "\n",
    "\n",
    "train_df[['e1_pos', 'e1_dep_noun', 'e1_dep_adj', 'e1_dep_verb', 'e1_dep_prep', 'e1_dep_subj', 'e1_dep_obj', 'e1_dep_token', 'e1_prev_token', 'e1_post_token', 'e1_start', 'e1_end', 'e1_position', 'e2_pos', 'e2_dep_noun', 'e2_dep_adj', 'e2_dep_verb', 'e2_dep_prep', 'e2_dep_subj', 'e2_dep_obj', 'e2_dep_token', 'e2_prev_token', 'e2_post_token', 'e2_start', 'e2_end', 'e2_position', 'num_verbs', 'num_nouns', 'num_prep', 'num_adj', 'num_words_btwn']] = train_df.apply(lambda x: extract_features(x['corrected_e1'],x['corrected_e2'],x['corrected_sentence']), axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8f4d20db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_sentence</th>\n",
       "      <th>entity_diff_vector</th>\n",
       "      <th>e1_supersense</th>\n",
       "      <th>e2_supersense</th>\n",
       "      <th>hypernym</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>[-0.030761719, 0.10595703, 0.14575195, -0.0831...</td>\n",
       "      <td>cognition</td>\n",
       "      <td>artifact</td>\n",
       "      <td>entity</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "      <td>[-0.005859375, 0.14013672, 0.021316528, 0.0341...</td>\n",
       "      <td>person</td>\n",
       "      <td>location</td>\n",
       "      <td>object</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the author of a keygen uses a disassembler to ...</td>\n",
       "      <td>[0.0234375, 0.23632812, -0.030822754, -0.03637...</td>\n",
       "      <td>person</td>\n",
       "      <td>quantity</td>\n",
       "      <td>entity</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "      <td>[0.19873047, 0.19555664, 0.09667969, -0.088989...</td>\n",
       "      <td>object</td>\n",
       "      <td>motion</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the student association is the voice of the un...</td>\n",
       "      <td>[-0.24389648, -0.30725098, -0.18603516, -0.241...</td>\n",
       "      <td>person</td>\n",
       "      <td>group</td>\n",
       "      <td>entity</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "      <td>[-0.06347656, -0.12963867, -0.080078125, 0.069...</td>\n",
       "      <td>communication</td>\n",
       "      <td>communication</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>the herbicide is derived from a natural antibi...</td>\n",
       "      <td>[-0.34570312, -0.09472656, 0.19384766, -0.3090...</td>\n",
       "      <td>substance</td>\n",
       "      <td>artifact</td>\n",
       "      <td>matter</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>to test this, we placed a kitchen match in the...</td>\n",
       "      <td>[0.27148438, 0.09118652, -0.020996094, 0.39208...</td>\n",
       "      <td>artifact</td>\n",
       "      <td>quantity</td>\n",
       "      <td>entity</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>[-0.33203125, -0.16259766, 0.017578125, -0.464...</td>\n",
       "      <td>person</td>\n",
       "      <td>artifact</td>\n",
       "      <td>whole</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>the surgeon cuts a small hole in the skull and...</td>\n",
       "      <td>[-0.24267578, -0.07714844, 0.0, 0.026611328, -...</td>\n",
       "      <td>person</td>\n",
       "      <td>contact</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          bert_sentence  \\\n",
       "0     the system as described above has its greatest...   \n",
       "1     the child was carefully wrapped and bound into...   \n",
       "2     the author of a keygen uses a disassembler to ...   \n",
       "3                 a misty ridge uprises from the surge.   \n",
       "4     the student association is the voice of the un...   \n",
       "...                                                 ...   \n",
       "7995  when the notice is sent by fax, the notice is ...   \n",
       "7996  the herbicide is derived from a natural antibi...   \n",
       "7997  to test this, we placed a kitchen match in the...   \n",
       "7998  the farmers and city officials in the region h...   \n",
       "7999  the surgeon cuts a small hole in the skull and...   \n",
       "\n",
       "                                     entity_diff_vector  e1_supersense  \\\n",
       "0     [-0.030761719, 0.10595703, 0.14575195, -0.0831...      cognition   \n",
       "1     [-0.005859375, 0.14013672, 0.021316528, 0.0341...         person   \n",
       "2     [0.0234375, 0.23632812, -0.030822754, -0.03637...         person   \n",
       "3     [0.19873047, 0.19555664, 0.09667969, -0.088989...         object   \n",
       "4     [-0.24389648, -0.30725098, -0.18603516, -0.241...         person   \n",
       "...                                                 ...            ...   \n",
       "7995  [-0.06347656, -0.12963867, -0.080078125, 0.069...  communication   \n",
       "7996  [-0.34570312, -0.09472656, 0.19384766, -0.3090...      substance   \n",
       "7997  [0.27148438, 0.09118652, -0.020996094, 0.39208...       artifact   \n",
       "7998  [-0.33203125, -0.16259766, 0.017578125, -0.464...         person   \n",
       "7999  [-0.24267578, -0.07714844, 0.0, 0.026611328, -...         person   \n",
       "\n",
       "      e2_supersense hypernym  relation  \n",
       "0          artifact   entity         3  \n",
       "1          location   object        18  \n",
       "2          quantity   entity        11  \n",
       "3            motion     None        18  \n",
       "4             group   entity        12  \n",
       "...             ...      ...       ...  \n",
       "7995  communication     None        18  \n",
       "7996       artifact   matter         8  \n",
       "7997       quantity   entity         6  \n",
       "7998       artifact    whole        18  \n",
       "7999        contact     None        17  \n",
       "\n",
       "[8000 rows x 6 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bert_sentence(new_sentence, e1, e2, e1_def, e2_def):\n",
    "#     new_sentence = new_sentence + ' what is the relationship between ' + e1 + ' and ' + e2 + ' where ' + e1 + ' is defined as ' + e1_def + ' and ' + e2 + ' is defined as ' + e2_def\n",
    "\n",
    "    return new_sentence\n",
    "\n",
    "train_df['bert_sentence'] = train_df.apply(lambda x: bert_sentence(x['corrected_sentence'],x['corrected_e1'],x['corrected_e2'],x['e1_definition'],x['e2_definition']),axis=1)\n",
    "# train_df = train_df[['new_sentence','entity_diff_vector','e1_supersense','e2_supersense','hypernym','relation']]\n",
    "train_df[['bert_sentence','entity_diff_vector','e1_supersense','e2_supersense','hypernym','relation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f391b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vectors(text):\n",
    "\n",
    "    try:\n",
    "        if text in word2vec_model:\n",
    "            return word2vec_model[text]\n",
    "\n",
    "        elif text.replace('-','') in word2vec_model:\n",
    "            return word2vec_model[text.replace('-','')]\n",
    "\n",
    "        elif text.replace('-',' ').split()[-1] in word2vec_model:\n",
    "            return word2vec_model[text.replace('-',' ').split()[-1]]\n",
    "\n",
    "        elif text.replace('-',' ').split()[0] in word2vec_model:\n",
    "            return word2vec_model[text.replace('-',' ').split()[0]]\n",
    "\n",
    "        elif spell_checker.correction(text) in word2vec_model:\n",
    "            return word2vec_model[spell_checker.correction(text)]\n",
    "\n",
    "        else:\n",
    "            return np.zeros(word2vec_model.vector_size)\n",
    "    except:\n",
    "        print (1, text, 2)\n",
    "\n",
    "train_df['e1_supersense'] = train_df['e1_supersense'].apply(text2vectors)\n",
    "train_df['e2_supersense'] = train_df['e2_supersense'].apply(text2vectors)\n",
    "train_df['hypernym'] = train_df['hypernym'].apply(text2vectors)\n",
    "train_df['e1_definition_embedding'] = train_df['e1_definition'].apply(text2vectors)\n",
    "train_df['e2_definition_embedding'] = train_df['e2_definition'].apply(text2vectors)\n",
    "train_df['e1_prev_token'] = train_df['e1_prev_token'].apply(text2vectors)\n",
    "train_df['e2_prev_token'] = train_df['e2_prev_token'].apply(text2vectors)\n",
    "train_df['e1_post_token'] = train_df['e1_post_token'].apply(text2vectors)\n",
    "train_df['e2_post_token'] = train_df['e2_post_token'].apply(text2vectors)\n",
    "train_df['e1_dep_token'] = train_df['e1_dep_token'].apply(text2vectors)\n",
    "train_df['e2_dep_token'] = train_df['e2_dep_token'].apply(text2vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "296eb67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected_sentence</th>\n",
       "      <th>entity_diff_vector</th>\n",
       "      <th>entity1_vectors</th>\n",
       "      <th>e1_supersense</th>\n",
       "      <th>e2_supersense</th>\n",
       "      <th>hypernym</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the system as described above has its greatest...</td>\n",
       "      <td>[-0.030761719, 0.10595703, 0.14575195, -0.0831...</td>\n",
       "      <td>[0.13964844, 0.07373047, -0.037841797, 0.10302...</td>\n",
       "      <td>[0.18066406, -0.0107421875, -0.044677734, 0.13...</td>\n",
       "      <td>[0.26953125, 0.12695312, -0.07470703, 0.051513...</td>\n",
       "      <td>[0.09326172, -0.43945312, 0.083984375, 0.04980...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "      <td>[-0.005859375, 0.14013672, 0.021316528, 0.0341...</td>\n",
       "      <td>[0.16503906, -0.063964844, -0.0017852783, 0.18...</td>\n",
       "      <td>[0.27539062, -0.24707031, 0.017211914, 0.16796...</td>\n",
       "      <td>[0.032714844, -0.096191406, 0.044189453, 0.173...</td>\n",
       "      <td>[0.29296875, -0.06298828, 0.083496094, 0.04345...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the author of a keygen uses a disassembler to ...</td>\n",
       "      <td>[0.0234375, 0.23632812, -0.030822754, -0.03637...</td>\n",
       "      <td>[0.12988281, -0.140625, 0.041748047, 0.0927734...</td>\n",
       "      <td>[0.27539062, -0.24707031, 0.017211914, 0.16796...</td>\n",
       "      <td>[0.0146484375, 0.13378906, 0.2734375, -0.01025...</td>\n",
       "      <td>[0.09326172, -0.43945312, 0.083984375, 0.04980...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "      <td>[0.19873047, 0.19555664, 0.09667969, -0.088989...</td>\n",
       "      <td>[-0.09472656, -0.059814453, -0.203125, 0.06298...</td>\n",
       "      <td>[0.29296875, -0.06298828, 0.083496094, 0.04345...</td>\n",
       "      <td>[0.068847656, 0.05078125, -0.19140625, 0.19628...</td>\n",
       "      <td>[-0.49414062, -0.12890625, 0.13476562, 0.02807...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the student association is the voice of the un...</td>\n",
       "      <td>[-0.24389648, -0.30725098, -0.18603516, -0.241...</td>\n",
       "      <td>[0.036865234, 0.020141602, 0.22167969, 0.15527...</td>\n",
       "      <td>[0.27539062, -0.24707031, 0.017211914, 0.16796...</td>\n",
       "      <td>[-0.021972656, 0.015197754, -0.029907227, 0.00...</td>\n",
       "      <td>[0.09326172, -0.43945312, 0.083984375, 0.04980...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "      <td>[-0.06347656, -0.12963867, -0.080078125, 0.069...</td>\n",
       "      <td>[-0.19824219, 0.08984375, 0.20898438, -0.16699...</td>\n",
       "      <td>[0.0026855469, -0.23339844, -0.080566406, -0.0...</td>\n",
       "      <td>[0.0026855469, -0.23339844, -0.080566406, -0.0...</td>\n",
       "      <td>[-0.49414062, -0.12890625, 0.13476562, 0.02807...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>the herbicide is derived from a natural antibi...</td>\n",
       "      <td>[-0.34570312, -0.09472656, 0.19384766, -0.3090...</td>\n",
       "      <td>[0.20019531, 0.29492188, -0.115234375, 0.26953...</td>\n",
       "      <td>[0.111328125, -0.013000488, 0.32421875, -0.192...</td>\n",
       "      <td>[0.26953125, 0.12695312, -0.07470703, 0.051513...</td>\n",
       "      <td>[0.107910156, 0.016601562, 0.076171875, 0.0202...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>to test this, we placed a kitchen match in the...</td>\n",
       "      <td>[0.27148438, 0.09118652, -0.020996094, 0.39208...</td>\n",
       "      <td>[-0.15527344, 0.025024414, 0.064941406, -0.124...</td>\n",
       "      <td>[0.26953125, 0.12695312, -0.07470703, 0.051513...</td>\n",
       "      <td>[0.0146484375, 0.13378906, 0.2734375, -0.01025...</td>\n",
       "      <td>[0.09326172, -0.43945312, 0.083984375, 0.04980...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>[-0.33203125, -0.16259766, 0.017578125, -0.464...</td>\n",
       "      <td>[0.17578125, 0.07470703, -0.24707031, 0.233398...</td>\n",
       "      <td>[0.27539062, -0.24707031, 0.017211914, 0.16796...</td>\n",
       "      <td>[0.26953125, 0.12695312, -0.07470703, 0.051513...</td>\n",
       "      <td>[0.07519531, -0.018920898, -0.0053710938, 0.23...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>the surgeon cuts a small hole in the skull and...</td>\n",
       "      <td>[-0.24267578, -0.07714844, 0.0, 0.026611328, -...</td>\n",
       "      <td>[0.100097656, 0.203125, 0.15527344, -0.0612792...</td>\n",
       "      <td>[0.27539062, -0.24707031, 0.017211914, 0.16796...</td>\n",
       "      <td>[0.046875, -0.23925781, -0.003036499, -0.22460...</td>\n",
       "      <td>[-0.49414062, -0.12890625, 0.13476562, 0.02807...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     corrected_sentence  \\\n",
       "0     the system as described above has its greatest...   \n",
       "1     the child was carefully wrapped and bound into...   \n",
       "2     the author of a keygen uses a disassembler to ...   \n",
       "3                 a misty ridge uprises from the surge.   \n",
       "4     the student association is the voice of the un...   \n",
       "...                                                 ...   \n",
       "7995  when the notice is sent by fax, the notice is ...   \n",
       "7996  the herbicide is derived from a natural antibi...   \n",
       "7997  to test this, we placed a kitchen match in the...   \n",
       "7998  the farmers and city officials in the region h...   \n",
       "7999  the surgeon cuts a small hole in the skull and...   \n",
       "\n",
       "                                     entity_diff_vector  \\\n",
       "0     [-0.030761719, 0.10595703, 0.14575195, -0.0831...   \n",
       "1     [-0.005859375, 0.14013672, 0.021316528, 0.0341...   \n",
       "2     [0.0234375, 0.23632812, -0.030822754, -0.03637...   \n",
       "3     [0.19873047, 0.19555664, 0.09667969, -0.088989...   \n",
       "4     [-0.24389648, -0.30725098, -0.18603516, -0.241...   \n",
       "...                                                 ...   \n",
       "7995  [-0.06347656, -0.12963867, -0.080078125, 0.069...   \n",
       "7996  [-0.34570312, -0.09472656, 0.19384766, -0.3090...   \n",
       "7997  [0.27148438, 0.09118652, -0.020996094, 0.39208...   \n",
       "7998  [-0.33203125, -0.16259766, 0.017578125, -0.464...   \n",
       "7999  [-0.24267578, -0.07714844, 0.0, 0.026611328, -...   \n",
       "\n",
       "                                        entity1_vectors  \\\n",
       "0     [0.13964844, 0.07373047, -0.037841797, 0.10302...   \n",
       "1     [0.16503906, -0.063964844, -0.0017852783, 0.18...   \n",
       "2     [0.12988281, -0.140625, 0.041748047, 0.0927734...   \n",
       "3     [-0.09472656, -0.059814453, -0.203125, 0.06298...   \n",
       "4     [0.036865234, 0.020141602, 0.22167969, 0.15527...   \n",
       "...                                                 ...   \n",
       "7995  [-0.19824219, 0.08984375, 0.20898438, -0.16699...   \n",
       "7996  [0.20019531, 0.29492188, -0.115234375, 0.26953...   \n",
       "7997  [-0.15527344, 0.025024414, 0.064941406, -0.124...   \n",
       "7998  [0.17578125, 0.07470703, -0.24707031, 0.233398...   \n",
       "7999  [0.100097656, 0.203125, 0.15527344, -0.0612792...   \n",
       "\n",
       "                                          e1_supersense  \\\n",
       "0     [0.18066406, -0.0107421875, -0.044677734, 0.13...   \n",
       "1     [0.27539062, -0.24707031, 0.017211914, 0.16796...   \n",
       "2     [0.27539062, -0.24707031, 0.017211914, 0.16796...   \n",
       "3     [0.29296875, -0.06298828, 0.083496094, 0.04345...   \n",
       "4     [0.27539062, -0.24707031, 0.017211914, 0.16796...   \n",
       "...                                                 ...   \n",
       "7995  [0.0026855469, -0.23339844, -0.080566406, -0.0...   \n",
       "7996  [0.111328125, -0.013000488, 0.32421875, -0.192...   \n",
       "7997  [0.26953125, 0.12695312, -0.07470703, 0.051513...   \n",
       "7998  [0.27539062, -0.24707031, 0.017211914, 0.16796...   \n",
       "7999  [0.27539062, -0.24707031, 0.017211914, 0.16796...   \n",
       "\n",
       "                                          e2_supersense  \\\n",
       "0     [0.26953125, 0.12695312, -0.07470703, 0.051513...   \n",
       "1     [0.032714844, -0.096191406, 0.044189453, 0.173...   \n",
       "2     [0.0146484375, 0.13378906, 0.2734375, -0.01025...   \n",
       "3     [0.068847656, 0.05078125, -0.19140625, 0.19628...   \n",
       "4     [-0.021972656, 0.015197754, -0.029907227, 0.00...   \n",
       "...                                                 ...   \n",
       "7995  [0.0026855469, -0.23339844, -0.080566406, -0.0...   \n",
       "7996  [0.26953125, 0.12695312, -0.07470703, 0.051513...   \n",
       "7997  [0.0146484375, 0.13378906, 0.2734375, -0.01025...   \n",
       "7998  [0.26953125, 0.12695312, -0.07470703, 0.051513...   \n",
       "7999  [0.046875, -0.23925781, -0.003036499, -0.22460...   \n",
       "\n",
       "                                               hypernym  relation  \n",
       "0     [0.09326172, -0.43945312, 0.083984375, 0.04980...         3  \n",
       "1     [0.29296875, -0.06298828, 0.083496094, 0.04345...        18  \n",
       "2     [0.09326172, -0.43945312, 0.083984375, 0.04980...        11  \n",
       "3     [-0.49414062, -0.12890625, 0.13476562, 0.02807...        18  \n",
       "4     [0.09326172, -0.43945312, 0.083984375, 0.04980...        12  \n",
       "...                                                 ...       ...  \n",
       "7995  [-0.49414062, -0.12890625, 0.13476562, 0.02807...        18  \n",
       "7996  [0.107910156, 0.016601562, 0.076171875, 0.0202...         8  \n",
       "7997  [0.09326172, -0.43945312, 0.083984375, 0.04980...         6  \n",
       "7998  [0.07519531, -0.018920898, -0.0053710938, 0.23...        18  \n",
       "7999  [-0.49414062, -0.12890625, 0.13476562, 0.02807...        17  \n",
       "\n",
       "[8000 rows x 7 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['corrected_sentence','entity_diff_vector','entity1_vectors','e1_supersense','e2_supersense','hypernym','relation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b45739ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df[train_df['flag']>0])\n",
    "# len(train_df[train_df['flag']>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d133e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['new_sentence_embedding'] = train_df['new_sentence'].apply(text_to_vectors)\n",
    "# train_df['e1_definition_embedding'] = train_df['e1_definition'].apply(text_to_vectors)\n",
    "# train_df['e2_definition_embedding'] = train_df['e2_definition'].apply(text_to_vectors)\n",
    "# X_entity1_positionstart_array = train_df['e1_start'].values.reshape(-1, 1)\n",
    "# train_df['sentence_embedding'] = train_df['sentence'].apply(text_to_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ff8ecb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN' 'PROPN' 'ADJ' 'VERB' None 'NUM' 'ADV' 'AUX']\n",
      "['NOUN' None 'ADJ' 'VERB' 'PROPN' 'ADV' 'DET' 'AUX']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "relation\n",
       "18    1394\n",
       "6      836\n",
       "1      653\n",
       "13     599\n",
       "8      558\n",
       "14     487\n",
       "3      464\n",
       "2      457\n",
       "11     397\n",
       "17     389\n",
       "4      372\n",
       "0      339\n",
       "16     317\n",
       "5      164\n",
       "9      146\n",
       "15     144\n",
       "10      97\n",
       "12      78\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (train_df['e1_pos'].unique())\n",
    "print (train_df['e2_pos'].unique())\n",
    "\n",
    "train_df_cpy = train_df.copy()\n",
    "train_df = train_df[(train_df['relation']!=7)&(train_df['flag']==0)]\n",
    "train_df['relation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4fb0b010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>relation</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>corrected_sentence</th>\n",
       "      <th>entity1_vectors</th>\n",
       "      <th>flag</th>\n",
       "      <th>entity2_vectors</th>\n",
       "      <th>corrected_e1</th>\n",
       "      <th>corrected_e2</th>\n",
       "      <th>...</th>\n",
       "      <th>e2_start</th>\n",
       "      <th>e2_end</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_prep</th>\n",
       "      <th>num_adj</th>\n",
       "      <th>num_words_btwn</th>\n",
       "      <th>e1_definition_embedding</th>\n",
       "      <th>e2_definition_embedding</th>\n",
       "      <th>bert_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the &lt;e1&gt;child&lt;/e1&gt; was carefully wrapped and b...</td>\n",
       "      <td>18</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "      <td>[0.16503906, -0.063964844, -0.0017852783, 0.18...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.15917969, 0.076171875, 0.01953125, 0.21875,...</td>\n",
       "      <td>child</td>\n",
       "      <td>cradle</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.15527344, 0.20898438, -0.15136719, -0.03271...</td>\n",
       "      <td>[0.063964844, -0.024536133, -0.033691406, 0.05...</td>\n",
       "      <td>the child was carefully wrapped and bound into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a misty &lt;e1&gt;ridge&lt;/e1&gt; uprises from the &lt;e2&gt;su...</td>\n",
       "      <td>18</td>\n",
       "      <td>ridge</td>\n",
       "      <td>surge</td>\n",
       "      <td>a misty ridge uprises from the surge.</td>\n",
       "      <td>[-0.09472656, -0.059814453, -0.203125, 0.06298...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.104003906, 0.13574219, -0.10644531, -0.0260...</td>\n",
       "      <td>ridge</td>\n",
       "      <td>surge</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.08691406, 0.24414062, 0.05834961, 0.0299072...</td>\n",
       "      <td>[0.028442383, 0.28710938, 0.033691406, -0.1904...</td>\n",
       "      <td>a misty ridge uprises from the surge. What is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>this is the sprawling &lt;e1&gt;complex&lt;/e1&gt; that is...</td>\n",
       "      <td>18</td>\n",
       "      <td>complex</td>\n",
       "      <td>producer</td>\n",
       "      <td>this is the sprawling complex that is peru's l...</td>\n",
       "      <td>[0.013977051, 0.08984375, -0.00062179565, 0.03...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.018188477, -0.2734375, -0.12792969, 0.0101...</td>\n",
       "      <td>complex</td>\n",
       "      <td>producer</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.13867188, 0.16503906, 0.1328125, 0.18066406...</td>\n",
       "      <td>[-0.0859375, -0.060791016, 0.0859375, -0.13769...</td>\n",
       "      <td>this is the sprawling complex that is peru's l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>their &lt;e1&gt;composer&lt;/e1&gt; has sunk into &lt;e2&gt;obli...</td>\n",
       "      <td>18</td>\n",
       "      <td>composer</td>\n",
       "      <td>oblivion</td>\n",
       "      <td>their composer has sunk into oblivion.</td>\n",
       "      <td>[0.22363281, -0.24902344, 0.03491211, -0.19238...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.18359375, -0.061035156, -0.084472656, 0.261...</td>\n",
       "      <td>composer</td>\n",
       "      <td>oblivion</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.1171875, -0.18066406, -0.17089844, 0.18164...</td>\n",
       "      <td>[0.09863281, 0.26757812, 0.14648438, 0.1269531...</td>\n",
       "      <td>their composer has sunk into oblivion. What is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>his intellectually engaging books and &lt;e1&gt;essa...</td>\n",
       "      <td>18</td>\n",
       "      <td>essays</td>\n",
       "      <td>history</td>\n",
       "      <td>his intellectually engaging books and essays r...</td>\n",
       "      <td>[-0.08251953, 0.10205078, 0.083496094, 0.21289...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.096191406, 0.13574219, 0.13574219, 0.115234...</td>\n",
       "      <td>essays</td>\n",
       "      <td>history</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.16015625, 0.119140625, 0.16503906, -0.10595...</td>\n",
       "      <td>[-0.045166016, -0.07519531, -0.009399414, -0.2...</td>\n",
       "      <td>his intellectually engaging books and essays r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>this is incorrect, and when a &lt;e1&gt;minicab&lt;/e1&gt;...</td>\n",
       "      <td>18</td>\n",
       "      <td>minicab</td>\n",
       "      <td>company</td>\n",
       "      <td>this is incorrect, and when a minicab company ...</td>\n",
       "      <td>[-0.19042969, -0.265625, 0.027954102, 0.060546...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.03564453, -0.13378906, -0.07324219, -0.093...</td>\n",
       "      <td>minicab</td>\n",
       "      <td>company</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.21679688, -0.1640625, -0.265625, -0.298828...</td>\n",
       "      <td>[0.30273438, -0.20214844, -0.05444336, 0.18554...</td>\n",
       "      <td>this is incorrect, and when a minicab company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>a &lt;e1&gt;facilitator&lt;/e1&gt; keeps the &lt;e2&gt;discussio...</td>\n",
       "      <td>18</td>\n",
       "      <td>facilitator</td>\n",
       "      <td>discussion</td>\n",
       "      <td>a facilitator keeps the discussion focused and...</td>\n",
       "      <td>[0.028564453, -0.21386719, 0.08105469, -0.1308...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0390625, 0.042236328, 0.014465332, 0.11523...</td>\n",
       "      <td>facilitator</td>\n",
       "      <td>discussion</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.040039062, -0.057861328, -0.12109375, 0.075...</td>\n",
       "      <td>[0.14941406, 0.01159668, -0.013305664, 0.26953...</td>\n",
       "      <td>a facilitator keeps the discussion focused and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7994</th>\n",
       "      <td>a v8 &lt;e1&gt;engine&lt;/e1&gt; mated with a manual trans...</td>\n",
       "      <td>18</td>\n",
       "      <td>engine</td>\n",
       "      <td>concept</td>\n",
       "      <td>a v8 engine mated with a manual transmission p...</td>\n",
       "      <td>[0.33789062, 0.008300781, 0.053222656, -0.0566...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.25585938, 0.03564453, 0.041992188, 0.193359...</td>\n",
       "      <td>engine</td>\n",
       "      <td>concept</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>[0.080566406, 0.044189453, 0.19824219, -0.0024...</td>\n",
       "      <td>[0.040771484, 0.21875, -0.1796875, 0.059570312...</td>\n",
       "      <td>a v8 engine mated with a manual transmission p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>when the &lt;e1&gt;notice&lt;/e1&gt; is sent by &lt;e2&gt;fax&lt;/e...</td>\n",
       "      <td>18</td>\n",
       "      <td>notice</td>\n",
       "      <td>fax</td>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "      <td>[-0.19824219, 0.08984375, 0.20898438, -0.16699...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.26171875, -0.039794922, 0.12890625, -0.097...</td>\n",
       "      <td>notice</td>\n",
       "      <td>fax</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>[-0.07128906, 0.0625, 0.10546875, 0.0019378662...</td>\n",
       "      <td>[0.25585938, -0.022094727, 0.029052734, 0.0544...</td>\n",
       "      <td>when the notice is sent by fax, the notice is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>18</td>\n",
       "      <td>farmers</td>\n",
       "      <td>market</td>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "      <td>[0.17578125, 0.07470703, -0.24707031, 0.233398...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.15625, -0.087890625, -0.22949219, -0.23144...</td>\n",
       "      <td>farmers</td>\n",
       "      <td>market</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>101</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>[0.12597656, 0.19042969, 0.06982422, 0.0722656...</td>\n",
       "      <td>[0.12011719, -0.020141602, 0.20703125, 0.14550...</td>\n",
       "      <td>the farmers and city officials in the region h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1394 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  relation  \\\n",
       "1     the <e1>child</e1> was carefully wrapped and b...        18   \n",
       "3     a misty <e1>ridge</e1> uprises from the <e2>su...        18   \n",
       "5     this is the sprawling <e1>complex</e1> that is...        18   \n",
       "11    their <e1>composer</e1> has sunk into <e2>obli...        18   \n",
       "23    his intellectually engaging books and <e1>essa...        18   \n",
       "...                                                 ...       ...   \n",
       "7980  this is incorrect, and when a <e1>minicab</e1>...        18   \n",
       "7982  a <e1>facilitator</e1> keeps the <e2>discussio...        18   \n",
       "7994  a v8 <e1>engine</e1> mated with a manual trans...        18   \n",
       "7995  when the <e1>notice</e1> is sent by <e2>fax</e...        18   \n",
       "7998  the farmers and city officials in the region h...        18   \n",
       "\n",
       "               e1          e2  \\\n",
       "1           child      cradle   \n",
       "3           ridge       surge   \n",
       "5         complex    producer   \n",
       "11       composer    oblivion   \n",
       "23         essays     history   \n",
       "...           ...         ...   \n",
       "7980      minicab     company   \n",
       "7982  facilitator  discussion   \n",
       "7994       engine     concept   \n",
       "7995       notice         fax   \n",
       "7998      farmers      market   \n",
       "\n",
       "                                     corrected_sentence  \\\n",
       "1     the child was carefully wrapped and bound into...   \n",
       "3                 a misty ridge uprises from the surge.   \n",
       "5     this is the sprawling complex that is peru's l...   \n",
       "11               their composer has sunk into oblivion.   \n",
       "23    his intellectually engaging books and essays r...   \n",
       "...                                                 ...   \n",
       "7980  this is incorrect, and when a minicab company ...   \n",
       "7982  a facilitator keeps the discussion focused and...   \n",
       "7994  a v8 engine mated with a manual transmission p...   \n",
       "7995  when the notice is sent by fax, the notice is ...   \n",
       "7998  the farmers and city officials in the region h...   \n",
       "\n",
       "                                        entity1_vectors  flag  \\\n",
       "1     [0.16503906, -0.063964844, -0.0017852783, 0.18...     0   \n",
       "3     [-0.09472656, -0.059814453, -0.203125, 0.06298...     0   \n",
       "5     [0.013977051, 0.08984375, -0.00062179565, 0.03...     0   \n",
       "11    [0.22363281, -0.24902344, 0.03491211, -0.19238...     0   \n",
       "23    [-0.08251953, 0.10205078, 0.083496094, 0.21289...     0   \n",
       "...                                                 ...   ...   \n",
       "7980  [-0.19042969, -0.265625, 0.027954102, 0.060546...     0   \n",
       "7982  [0.028564453, -0.21386719, 0.08105469, -0.1308...     0   \n",
       "7994  [0.33789062, 0.008300781, 0.053222656, -0.0566...     0   \n",
       "7995  [-0.19824219, 0.08984375, 0.20898438, -0.16699...     0   \n",
       "7998  [0.17578125, 0.07470703, -0.24707031, 0.233398...     0   \n",
       "\n",
       "                                        entity2_vectors corrected_e1  \\\n",
       "1     [0.15917969, 0.076171875, 0.01953125, 0.21875,...        child   \n",
       "3     [0.104003906, 0.13574219, -0.10644531, -0.0260...        ridge   \n",
       "5     [-0.018188477, -0.2734375, -0.12792969, 0.0101...      complex   \n",
       "11    [0.18359375, -0.061035156, -0.084472656, 0.261...     composer   \n",
       "23    [0.096191406, 0.13574219, 0.13574219, 0.115234...       essays   \n",
       "...                                                 ...          ...   \n",
       "7980  [-0.03564453, -0.13378906, -0.07324219, -0.093...      minicab   \n",
       "7982  [-0.0390625, 0.042236328, 0.014465332, 0.11523...  facilitator   \n",
       "7994  [0.25585938, 0.03564453, 0.041992188, 0.193359...       engine   \n",
       "7995  [-0.26171875, -0.039794922, 0.12890625, -0.097...       notice   \n",
       "7998  [-0.15625, -0.087890625, -0.22949219, -0.23144...      farmers   \n",
       "\n",
       "     corrected_e2  ... e2_start e2_end num_verbs num_nouns num_prep num_adj  \\\n",
       "1          cradle  ...       51     57         2         1        1       0   \n",
       "3           surge  ...       31     36         0         2        1       0   \n",
       "5        producer  ...       53     61         0         1        0       1   \n",
       "11       oblivion  ...       29     37         1         1        1       0   \n",
       "23        history  ...       91     98         2         1        1       2   \n",
       "...           ...  ...      ...    ...       ...       ...      ...     ...   \n",
       "7980      company  ...       38     45         0         1        0       0   \n",
       "7982   discussion  ...       24     34         1         1        0       0   \n",
       "7994      concept  ...       68     75         1         3        1       2   \n",
       "7995          fax  ...       27     30         4         3        3       0   \n",
       "7998       market  ...       95    101         3        11        2       1   \n",
       "\n",
       "     num_words_btwn                            e1_definition_embedding  \\\n",
       "1                 8  [0.15527344, 0.20898438, -0.15136719, -0.03271...   \n",
       "3                 4  [0.08691406, 0.24414062, 0.05834961, 0.0299072...   \n",
       "5                 6  [0.13867188, 0.16503906, 0.1328125, 0.18066406...   \n",
       "11                4  [-0.1171875, -0.18066406, -0.17089844, 0.18164...   \n",
       "23                6  [0.16015625, 0.119140625, 0.16503906, -0.10595...   \n",
       "...             ...                                                ...   \n",
       "7980              1  [-0.21679688, -0.1640625, -0.265625, -0.298828...   \n",
       "7982              3  [0.040039062, -0.057861328, -0.12109375, 0.075...   \n",
       "7994              9  [0.080566406, 0.044189453, 0.19824219, -0.0024...   \n",
       "7995             16  [-0.07128906, 0.0625, 0.10546875, 0.0019378662...   \n",
       "7998             27  [0.12597656, 0.19042969, 0.06982422, 0.0722656...   \n",
       "\n",
       "                                e2_definition_embedding  \\\n",
       "1     [0.063964844, -0.024536133, -0.033691406, 0.05...   \n",
       "3     [0.028442383, 0.28710938, 0.033691406, -0.1904...   \n",
       "5     [-0.0859375, -0.060791016, 0.0859375, -0.13769...   \n",
       "11    [0.09863281, 0.26757812, 0.14648438, 0.1269531...   \n",
       "23    [-0.045166016, -0.07519531, -0.009399414, -0.2...   \n",
       "...                                                 ...   \n",
       "7980  [0.30273438, -0.20214844, -0.05444336, 0.18554...   \n",
       "7982  [0.14941406, 0.01159668, -0.013305664, 0.26953...   \n",
       "7994  [0.040771484, 0.21875, -0.1796875, 0.059570312...   \n",
       "7995  [0.25585938, -0.022094727, 0.029052734, 0.0544...   \n",
       "7998  [0.12011719, -0.020141602, 0.20703125, 0.14550...   \n",
       "\n",
       "                                          bert_sentence  \n",
       "1     the child was carefully wrapped and bound into...  \n",
       "3     a misty ridge uprises from the surge. What is ...  \n",
       "5     this is the sprawling complex that is peru's l...  \n",
       "11    their composer has sunk into oblivion. What is...  \n",
       "23    his intellectually engaging books and essays r...  \n",
       "...                                                 ...  \n",
       "7980  this is incorrect, and when a minicab company ...  \n",
       "7982  a facilitator keeps the discussion focused and...  \n",
       "7994  a v8 engine mated with a manual transmission p...  \n",
       "7995  when the notice is sent by fax, the notice is ...  \n",
       "7998  the farmers and city officials in the region h...  \n",
       "\n",
       "[1394 rows x 44 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['relation']==18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624150f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cpu\n",
      "1\n",
      "Initializing Bert distilbert-base-uncased\n",
      "Vectorization done on cpu\n"
     ]
    }
   ],
   "source": [
    "sentence_vector=[]\n",
    "batch_size = 500\n",
    "for i in range(int(np.ceil(len(train_df)/batch_size))):\n",
    "    print (i)\n",
    "    vectorizer = Vectorizer()\n",
    "    vectorizer.run(train_df[batch_size*i:batch_size*i+batch_size]['bert_sentence'].tolist())\n",
    "    vectors = vectorizer.vectors\n",
    "    sentence_vector = sentence_vector+vectors\n",
    "new_sentence_embedding_new = np.vstack(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "af9838a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulsen/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/rahulsen/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((new_sentence_embedding_new, entity1_vectors_new, entity2_vectors_new, entity_diff_vector_new, hypernym_new, e1_dep_token, e2_dep_token, e1_prev_token, e1_post_token, e2_prev_token, e2_post_token, e1_supersense_new, e2_supersense_new,num_words_btwn, num_prep), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     61\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 63\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Train SVM model\u001b[39;00m\n\u001b[1;32m     66\u001b[0m svm_model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2638\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2634\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2636\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2638\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2641\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2643\u001b[0m     )\n\u001b[1;32m   2644\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:1726\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1726\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2115\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2113\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2120\u001b[0m     )\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2126\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate vectors from all batches\n",
    "# new_sentence_embedding_new = np.vstack(vectors)\n",
    "\n",
    "# new_sentence_embedding_new = np.vstack(train_df['new_sentence_embedding'])\n",
    "e1_definition_embedding_new = np.vstack(train_df['e1_definition_embedding'])\n",
    "e2_definition_embedding_new = np.vstack(train_df['e2_definition_embedding'])\n",
    "entity_diff_vector_new = np.vstack(train_df['entity_diff_vector'])\n",
    "entity1_vectors_new = np.vstack(train_df['entity1_vectors'])\n",
    "entity2_vectors_new = np.vstack(train_df['entity2_vectors'])\n",
    "e1_supersense_new = np.vstack(train_df['e1_supersense'])\n",
    "e2_supersense_new = np.vstack(train_df['e2_supersense'])\n",
    "hypernym_new = np.vstack(train_df['hypernym'])\n",
    "e1_dep_token = np.vstack(train_df['e1_dep_token'])\n",
    "e2_dep_token = np.vstack(train_df['e2_dep_token'])\n",
    "e1_prev_token = np.vstack(train_df['e1_prev_token'])\n",
    "e2_prev_token = np.vstack(train_df['e2_prev_token'])\n",
    "e1_post_token = np.vstack(train_df['e1_post_token'])\n",
    "e2_post_token = np.vstack(train_df['e2_post_token'])\n",
    "\n",
    "onehot_encoder_e1_pos = OneHotEncoder(sparse=False)\n",
    "e1_pos = onehot_encoder_e1_pos.fit_transform(np.array(train_df['e1_pos']).reshape(-1,1))\n",
    "\n",
    "e1_dep_noun = np.array(train_df['e1_dep_noun']).reshape(-1,1)\n",
    "e1_dep_adj = np.array(train_df['e1_dep_adj']).reshape(-1,1)\n",
    "e1_dep_verb = np.array(train_df['e1_dep_verb']).reshape(-1,1)\n",
    "e1_dep_prep = np.array(train_df['e1_dep_prep']).reshape(-1,1)\n",
    "e1_dep_subj = np.array(train_df['e1_dep_subj']).reshape(-1,1)\n",
    "e1_dep_obj = np.array(train_df['e1_dep_obj']).reshape(-1,1)\n",
    "\n",
    "e1_start = np.array(train_df['e1_start']).reshape(-1,1)\n",
    "e1_end = np.array(train_df['e1_end']).reshape(-1,1)\n",
    "\n",
    "# e2_pos = np.array(train_df['e2_pos']).reshape(-1,1)\n",
    "onehot_encoder_e2_pos = OneHotEncoder(sparse=False)\n",
    "e2_pos = onehot_encoder_e2_pos.fit_transform(np.array(train_df['e2_pos']).reshape(-1,1))\n",
    "\n",
    "e2_dep_noun = np.array(train_df['e2_dep_noun']).reshape(-1,1)\n",
    "e2_dep_adj = np.array(train_df['e2_dep_adj']).reshape(-1,1)\n",
    "e2_dep_verb = np.array(train_df['e2_dep_verb']).reshape(-1,1)\n",
    "e2_dep_prep = np.array(train_df['e2_dep_prep']).reshape(-1,1)\n",
    "e2_dep_subj = np.array(train_df['e2_dep_subj']).reshape(-1,1)\n",
    "e2_dep_obj = np.array(train_df['e2_dep_obj']).reshape(-1,1)\n",
    "\n",
    "e2_start = np.array(train_df['e2_start']).reshape(-1,1)\n",
    "e2_end = np.array(train_df['e2_end']).reshape(-1,1)\n",
    "num_verbs = np.array(train_df['num_verbs']).reshape(-1,1)\n",
    "num_nouns = np.array(train_df['num_nouns']).reshape(-1,1)\n",
    "num_prep = np.array(train_df['num_prep']).reshape(-1,1)\n",
    "num_adj = np.array(train_df['num_adj']).reshape(-1,1)\n",
    "num_words_btwn = np.array(train_df['num_words_btwn']).reshape(-1,1) \n",
    "\n",
    "\n",
    "\n",
    "# X = np.concatenate(( new_sentence_embedding_new, entity1_vectors_new, entity2_vectors_new, entity_diff_vector_new, e1_supersense_new, e2_supersense_new, hypernym_new, e1_pos,e1_dep_noun,e1_dep_adj,e1_dep_verb,e1_dep_prep,e1_dep_subj,e1_dep_obj,e1_start,e1_end,e2_pos,e2_dep_adj,e2_dep_verb,e2_dep_prep,e2_dep_subj,e2_dep_obj,e2_start,e2_end,num_verbs,num_nouns,num_prep,num_adj,num_words_btwn), axis=1)\n",
    "# X = np.concatenate((new_sentence_embedding_new, hypernym_new, entity_diff_vector_new, e1_supersense_new, e2_supersense_new, e1_dep_noun,e1_dep_adj,e1_dep_verb,e1_dep_prep,e2_dep_noun, e2_dep_adj,e2_dep_verb,e2_dep_prep,num_words_btwn), axis=1)\n",
    "X = np.concatenate((new_sentence_embedding_new, entity1_vectors_new, entity2_vectors_new, entity_diff_vector_new, hypernym_new, e1_dep_token, e2_dep_token, e1_prev_token, e1_post_token, e2_prev_token, e2_post_token, e1_supersense_new, e2_supersense_new,num_words_btwn, num_prep), axis=1)\n",
    "y = np.array(train_df['relation'].tolist())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# # Model evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b751041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# print (np.array(sentence_vector).shape)\n",
    "print (len(new_sentence_embedding_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1b864800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Epoch 1/20, Average Loss: 1.8512\n",
      "Epoch 2/20, Average Loss: 1.0340\n",
      "Epoch 3/20, Average Loss: 0.8037\n",
      "Epoch 4/20, Average Loss: 0.6690\n",
      "Epoch 5/20, Average Loss: 0.5740\n",
      "Epoch 6/20, Average Loss: 0.4993\n",
      "Epoch 7/20, Average Loss: 0.4352\n",
      "Epoch 8/20, Average Loss: 0.3753\n",
      "Epoch 9/20, Average Loss: 0.3269\n",
      "Epoch 10/20, Average Loss: 0.2824\n",
      "Epoch 11/20, Average Loss: 0.2486\n",
      "Epoch 12/20, Average Loss: 0.2110\n",
      "Epoch 13/20, Average Loss: 0.1882\n",
      "Epoch 14/20, Average Loss: 0.1577\n",
      "Epoch 15/20, Average Loss: 0.1328\n",
      "Epoch 16/20, Average Loss: 0.1121\n",
      "Epoch 17/20, Average Loss: 0.0976\n",
      "Epoch 18/20, Average Loss: 0.0821\n",
      "Epoch 19/20, Average Loss: 0.0699\n",
      "Epoch 20/20, Average Loss: 0.0594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# Define your neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set hyperparameters\n",
    "input_dim = X_train.shape[1]  # Dimension of input features\n",
    "hidden_dim = 128  # Number of units in the hidden layer\n",
    "output_dim = 19#X_train.shape[1] #len(y_train.unique())  # Number of output classes\n",
    "print (output_dim)\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetwork(input_dim, hidden_dim, output_dim+1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Create a PyTorch DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "#         batch_labels = batch_labels.float()\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print average training loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Convert validation data to PyTorch tensors\n",
    "X_val_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Create a PyTorch DataLoader for validation data\n",
    "val_dataset = TensorDataset(X_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on validation data\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_inputs in val_dataloader:\n",
    "        outputs = model(batch_inputs[0])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.tolist())\n",
    "\n",
    "# Convert predictions to a pandas Series\n",
    "predictions_series = pd.Series(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "03899e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84        68\n",
      "           1       0.80      0.81      0.80       131\n",
      "           2       0.63      0.81      0.71        91\n",
      "           3       0.68      0.56      0.61        93\n",
      "           4       0.78      0.88      0.83        74\n",
      "           5       0.91      0.94      0.93        33\n",
      "           6       0.86      0.86      0.86       167\n",
      "           8       0.75      0.68      0.71       112\n",
      "           9       0.87      0.90      0.88        29\n",
      "          10       0.75      0.16      0.26        19\n",
      "          11       0.73      0.73      0.73        79\n",
      "          12       0.89      0.50      0.64        16\n",
      "          13       0.89      0.86      0.87       120\n",
      "          14       0.80      0.76      0.77        98\n",
      "          15       0.88      0.72      0.79        29\n",
      "          16       0.67      0.71      0.69        63\n",
      "          17       0.84      0.67      0.74        78\n",
      "          18       0.47      0.54      0.51       279\n",
      "\n",
      "    accuracy                           0.72      1579\n",
      "   macro avg       0.78      0.72      0.73      1579\n",
      "weighted avg       0.74      0.72      0.72      1579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_test, predictions_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Concatenate, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the relation column\n",
    "# y_onehot = to_categorical(train_df['relation'])\n",
    "\n",
    "# max_length = max(sentence_embeddings.shape[0], entity1_embeddings.shape[0], entity2_embeddings.shape[0])\n",
    "\n",
    "# # Pad embeddings arrays to the same length\n",
    "# sentence_embeddings_padded = np.pad(sentence_embeddings, ((0, max_length - sentence_embeddings.shape[0]), (0, 0)), mode='constant')\n",
    "# entity1_embeddings_padded = np.pad(entity1_embeddings, ((0, max_length - entity1_embeddings.shape[0]), (0, 0)), mode='constant')\n",
    "# entity2_embeddings_padded = np.pad(entity2_embeddings, ((0, max_length - entity2_embeddings.shape[0]), (0, 0)), mode='constant')\n",
    "\n",
    "# # Concatenate embeddings arrays along axis=1\n",
    "# features = np.concatenate([sentence_embeddings_padded, entity1_embeddings_padded, entity2_embeddings_padded], axis=1)\n",
    "\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, df['relation'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define CNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(features.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(19, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy:', test_acc)\n",
    "# Predictions\n",
    "predictions_onehot = model.predict(X_test)\n",
    "predictions_onehot\n",
    "# predictions_labels = label_encoder.inverse_transform(predictions_onehot.argmax(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08b284a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e1_text': 'configuration', 'e2_text': 'elements', 'context': 'The system as described above has its greatest application in an arrayed  of antenna .'}\n"
     ]
    }
   ],
   "source": [
    "relation_name = {\n",
    "0: 'Cause-Effect(e1,e2)',\n",
    "1: 'Cause-Effect(e2,e1)',\n",
    "2: 'Component-Whole(e1,e2)',\n",
    "3: 'Component-Whole(e2,e1)',\n",
    "4: 'Content-Container(e1,e2)',\n",
    "5: 'Content-Container(e2,e1)',\n",
    "6: 'Entity-Destination(e1,e2)',\n",
    "7: 'Entity-Destination(e2,e1)',\n",
    "8: 'Entity-Origin(e1,e2)',\n",
    "9: 'Entity-Origin(e2,e1)',\n",
    "10: 'Instrument-Agency(e1,e2)', \n",
    "11: 'Instrument-Agency(e2,e1)',\n",
    "12: 'Member-Collection(e1,e2)',\n",
    "13: 'Member-Collection(e2,e1)',\n",
    "14: 'Message-Topic(e1,e2)',\n",
    "15: 'Message-Topic(e2,e1)',\n",
    "16: 'Product-Producer(e1,e2)',\n",
    "17: 'Product-Producer(e2,e1)',\n",
    "18: 'Other'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
